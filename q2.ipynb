{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "title"},
      "source": [
        "# Q2 â€” Text-Driven Image Segmentation with SAM 2\n",
        "\n",
        "Pipeline: Load image -> accept text prompt -> convert text to region seeds (via CLIPSeg) -> feed seeds to SAM 2 -> display final mask overlay.\n",
        "\n",
        "This notebook is designed to run on Colab (GPU). If SAM 2 API or checkpoints change, adjust the install/checkpoint cell accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "installs"},
      "outputs": [],
      "source": [
        "%pip -q install opencv-python matplotlib pillow einops --upgrade\n",
        "# Attempt to install SAM 2 (adjust if the official package/repo name changes)\n",
        "%pip -q install git+https://github.com/facebookresearch/segment-anything-2.git || echo 'If this fails, update to the latest official SAM 2 install instructions.'\n",
        "%pip -q install clipseg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "imports"},
      "outputs": [],
      "source": [
        "import os, cv2, torch, numpy as np, matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "\n",
        "# CLIPSeg imports\n",
        "try:\n",
        "    import clipseg\n",
        "    from clipseg.models.clipseg import CLIPDensePredT\n",
        "except Exception as e:\n",
        "    print('CLIPSeg not available:', e)\n",
        "    CLIPDensePredT = None\n",
        "\n",
        "# SAM 2 imports (placeholder; update if the API changes)\n",
        "try:\n",
        "    import sam2  # placeholder for SAM 2 package\n",
        "except Exception as e:\n",
        "    print('SAM 2 not available yet. Please update install cell to latest official instructions.', e)\n",
        "    sam2 = None\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "helpers"},
      "outputs": [],
      "source": [
        "def load_image(path_or_url):\n",
        "    if path_or_url.startswith('http://') or path_or_url.startswith('https://'):\n",
        "        import requests, io\n",
        "        img = Image.open(io.BytesIO(requests.get(path_or_url).content)).convert('RGB')\n",
        "    else:\n",
        "        img = Image.open(path_or_url).convert('RGB')\n",
        "    return img\n",
        "\n",
        "def show_overlay(img_pil, mask_np, alpha=0.6):\n",
        "    img = np.array(img_pil)\n",
        "    mask = (mask_np > 0).astype(np.uint8)\n",
        "    color = np.array([30, 144, 255], dtype=np.uint8)\n",
        "    overlay = img.copy()\n",
        "    overlay[mask==1] = (alpha*color + (1-alpha)*overlay[mask==1]).astype(np.uint8)\n",
        "    plt.figure(figsize=(6,6)); plt.imshow(overlay); plt.axis('off'); plt.show()\n",
        "\n",
        "def text_to_heatmap_clipseg(img_pil, text):\n",
        "    assert CLIPDensePredT is not None, 'CLIPSeg not installed'\n",
        "    model = CLIPDensePredT(version='ViT-B/16', reduce_dim=64)\n",
        "    model.eval(); model.to(device)\n",
        "    model.load_state_dict(torch.hub.load_state_dict_from_url('https://huggingface.co/timojl/clipseg/resolve/main/rd64-uni.pth', map_location=device))\n",
        "    tf = T.Compose([T.Resize((352,352)), T.ToTensor(), T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
        "    x = tf(img_pil).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        preds = model(x, [text])[0]  # (1, 1, H, W)\n",
        "        heat = torch.sigmoid(preds).squeeze().cpu().numpy()\n",
        "    # resize back to image size\n",
        "    heat = cv2.resize(heat, img_pil.size, interpolation=cv2.INTER_LINEAR)\n",
        "    return heat\n",
        "\n",
        "def heatmap_to_point_seeds(heat, k=10, thresh=0.5):\n",
        "    pts = []\n",
        "    h, w = heat.shape\n",
        "    mask = (heat >= thresh).astype(np.uint8)\n",
        "    ys, xs = np.where(mask > 0)\n",
        "    if len(xs) == 0:\n",
        "        # fallback: take top-k maxima\n",
        "        flat_idx = np.argpartition(heat.flatten(), -k)[-k:]\n",
        "        ys, xs = np.unravel_index(flat_idx, (h,w))\n",
        "    for y,x in zip(ys, xs):\n",
        "        pts.append((int(x), int(y)))\n",
        "    # sub-sample if too many\n",
        "    if len(pts) > k:\n",
        "        idx = np.linspace(0, len(pts)-1, k).astype(int)\n",
        "        pts = [pts[i] for i in idx]\n",
        "    return pts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "io"},
      "outputs": [],
      "source": [
        "# Provide either a URL or upload a local image in Colab\n",
        "image_url = ''  # e.g., 'https://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "text_prompt = 'a dog'  # edit your prompt\n",
        "\n",
        "img = None\n",
        "if image_url:\n",
        "    img = load_image(image_url)\n",
        "else:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        up = files.upload()\n",
        "        if len(up):\n",
        "            fname = list(up.keys())[0]\n",
        "            img = load_image(fname)\n",
        "    except Exception:\n",
        "        pass\n",
        "assert img is not None, 'Please provide an image URL or upload an image.'\n",
        "plt.figure(figsize=(6,6)); plt.imshow(img); plt.axis('off'); plt.title('Input Image'); plt.show()\n",
        "\n",
        "# text->heatmap and seeds\n",
        "heat = text_to_heatmap_clipseg(img, text_prompt)\n",
        "plt.figure(figsize=(6,6)); plt.imshow(heat, cmap='magma'); plt.colorbar(); plt.title('CLIPSeg heatmap'); plt.axis('off'); plt.show()\n",
        "seeds = heatmap_to_point_seeds(heat, k=10, thresh=0.6)\n",
        "print('Seeds (x,y):', seeds[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "sam2"},
      "outputs": [],
      "source": [
        "# SAM 2 segmentation: placeholder API (update with the official predictor and checkpoint)\n",
        "if sam2 is None:\n",
        "    print('SAM 2 is not installed. Please update the install cell with the official package and checkpoint.')\n",
        "    # As a minimal fallback, visualize thresholded heatmap directly\n",
        "    mask = (heat >= 0.6).astype(np.uint8)\n",
        "    show_overlay(img, mask)\n",
        "else:\n",
        "    # TODO: Replace with official SAM 2 model loading and prediction using point prompts (seeds)\n",
        "    # Example (pseudo-code):\n",
        "    # checkpoint_path = '/content/sam2_checkpoint.pth'  # provide valid path or download\n",
        "    # predictor = sam2.SAM2Predictor(checkpoint=checkpoint_path)\n",
        "    # points = np.array([[x,y] for (x,y) in seeds], dtype=np.int32)\n",
        "    # labels = np.ones((points.shape[0],), dtype=np.int32)\n",
        "    # mask = predictor.predict_with_points(image=np.array(img), points=points, labels=labels)\n",
        "    # show_overlay(img, mask)\n",
        "    print('Please wire up the official SAM 2 predictor with the seeds. Showing heatmap fallback.')\n",
        "    mask = (heat >= 0.6).astype(np.uint8)\n",
        "    show_overlay(img, mask)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {"name": "q2.ipynb"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
